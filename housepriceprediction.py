# -*- coding: utf-8 -*-
"""HOUSEPRICEPREDICTION.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1jxvtwWzZ1Nh0EJrqzLy6loq00R2ynmDg
"""

#loading libraries
import pandas as pd
import numpy as np
import seaborn as sns
import matplotlib.pyplot as plt
from scipy import stats
from xgboost import XGBRegressor
import joblib

#preprocessing
from sklearn.preprocessing import LabelEncoder
from sklearn.model_selection import train_test_split
from sklearn.metrics import mean_absolute_error, r2_score


df = pd.read_csv('/content/train.csv')
df

"""### Missing values"""

missing_values = df.isnull().sum().sort_values(ascending=False)
missing_values

missing_percent = (df.isnull().mean() * 100).sort_values(ascending=False)
missing_percent

df = df.drop(columns=['Id', 'PoolQC', 'MiscFeature', 'Alley', 'Fence','MiscFeature'])
df

# Handle categorical columns in dataset
c_columns = df.select_dtypes(include=['object', 'category']).columns
c_columns

label_encoder = LabelEncoder()
for col in c_columns:
    df[col] = label_encoder.fit_transform(df[col])
df

# find correlation between these features
corr = df.corr()
corr_with_price = corr['SalePrice'].sort_values(ascending=False)
print("Correlated features", corr_with_price.head(20))

c_features = corr_with_price[corr_with_price>0.25].index
df = df[c_features]
df

plt.figure(figsize=(10,8))
sns.heatmap(df[corr_with_price[corr_with_price>0.25].index].corr(), annot=True, fmt='.2f', cmap='Reds')
plt.title("Correlation Heatmap of features")
plt.show()

df = df.drop(columns=['GarageCars', 'GarageYrBlt', 'TotRmsAbvGrd', 'Foundation', '2ndFlrSF', 'HalfBath', 'TotalBsmtSF'])

df.isnull().sum()

df['LotFrontage'] = df['LotFrontage'].fillna(df['LotFrontage'].mean())

x = df.drop(columns=['SalePrice'])
y = df['SalePrice']

from sklearn.model_selection import train_test_split
X_train, X_test, y_train, y_test = train_test_split(x, y, test_size=0.2, random_state=42)

model = XGBRegressor(
    objective='reg:squarederror',
    learning_rate = 0.01,
    max_depth = 6,
    n_estimators = 1000,
    subsample= 0.65,
    random_state = 42
)
model.fit(X_train, y_train)

y_pred = model.predict(X_test)

mae = mean_absolute_error(y_test, y_pred)
print("Mean Absolute Error:", mae)

r2score = r2_score(y_test, y_pred)
print("R2 Score:", r2score)

# MAE - Mean Absolute Error (average error) # i.e 12 - 9 (it's differance which is 3 )
# MSE - Mean Squared Error (average square error) # jitna bara error hoga utni bari value error ko square root me dega 3(2)
# RMSE - Root Mean Squared Error (root of mse)
# R2 - R squared (model variance)  #check compliance variance graph se line to kitna match krrha tha

#gradiant bosting ki acc is better than random forest cuz random fme learning share nhi horhi.
# gradiant bosting apni learning share krrha hai agay iske pass raw data bhi hai aur previous data bhi

df.columns

joblib.dump(model, 'xgb_model.jb')

from google.colab import files
files.download("xgb_model.jb")